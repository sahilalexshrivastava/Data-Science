{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Science Project\n",
        "## Team ASK\n",
        "### Andrei Kondyrev ID: 011738571\n",
        "### Sahil Shrivastava ID: 011717650\n",
        "### Kulpreet Singh ID: 011771817"
      ],
      "metadata": {
        "id": "XIy-396omxLt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XjqtNX8Yb81",
        "outputId": "d13e8eee-34a7-4ec5-a5a8-e4e901b16c19"
      },
      "source": [
        "# Libraries being used for data processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from pymystem3 import Mystem\n",
        "from string import punctuation\n",
        "\n",
        "# These are modules from Natural Language Toolkit library. Vader Lexicon is used for the sentiments part.\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "vds = SentimentIntensityAnalyzer()\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "token = WordPunctTokenizer()\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caBJCritCCUM",
        "outputId": "e1531c09-6a44-4691-a40e-5cb6e4320d6e"
      },
      "source": [
        "# This is just a code needed for the usage of Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8XBi92hY1Ra"
      },
      "source": [
        "# Reading the code and some initial processing\n",
        "twits = pd.read_csv(\"drive/MyDrive/WSU/Twitter/Andrei/Andrei.csv\")\n",
        "twits = twits.rename(columns = {\"Unnamed: 0\":\"Wrong\"})\n",
        "twits = twits.drop(twits.columns[0], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SOzkeBFY1Tk"
      },
      "source": [
        "# Separate function to get dataframes of tweets for each day\n",
        "def separate(twits, i):\n",
        "  if i < 8:\n",
        "    twits = twits[(twits['Time']>='2020-04-0{} 00:00:00'.format(i+1)) & (twits['Time']<'2020-04-0{} 00:00:00'.format(i+2))]\n",
        "  elif i == 8:\n",
        "    twits = twits[(twits['Time']>='2020-04-0{} 00:00:00'.format(i+1)) & (twits['Time']<'2020-04-{} 00:00:00'.format(i+2))]\n",
        "  else:\n",
        "    twits = twits[(twits['Time']>='2020-04-{} 00:00:00'.format(i+1)) & (twits['Time']<'2020-04-{} 00:00:00'.format(i+2))]\n",
        "\n",
        "  twits = twits.drop(twits.columns[0], axis = 1)\n",
        "  twits = twits.dropna(axis = 0)\n",
        "  twits = twits.reset_index(drop = True)\n",
        "\n",
        "  return twits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uobGOro5Y1Vt"
      },
      "source": [
        "# Assignment for the arrays used\n",
        "twitsDays = [[] for _ in range(16)]\n",
        "twitsCleaned = [[] for _ in range(16)]\n",
        "string = [[] for _ in range(16)]\n",
        "top50 = [[] for _ in range(16)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJOiG6FrwW1V"
      },
      "source": [
        "# Here are some regular expressions used for the data cleaning part.\n",
        "regex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "\n",
        "linkpatternH = re.compile(r\"http\\S+\")\n",
        "linkpatternW = re.compile(r\"www.\\S+\")\n",
        "emojipattern = re.compile(r\"emoji\\S+\")\n",
        "linkimage = re.compile(r\"pic.twitter.\\S+\")\n",
        "mention = '@[A-Za-z0-9_]+'\n",
        "hashtag = '#[A-Za-z0-9_]+'\n",
        "punctuation = re.compile(r\"[^\\w\\s]\")\n",
        "numbers = re.compile(r\"[\\d-]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bs4bqkjZY1XT"
      },
      "source": [
        "# This is a cleaning function. We are passing one tweet at a time. The process\n",
        "# of cleaning starts with lowering the case of letters. Then we exclude picture\n",
        "# links, http and www links, getting rid of URL-codes (like %20 for space),\n",
        "# mentions and of all the punctuation.\n",
        "# Then we tokenize the words and exclude all words of length less than 2.\n",
        "# Finally, the function returns a string of cleaned and tokenized words to\n",
        "# generalize on classification and wordclouds better.\n",
        "def clean(t):\n",
        "\n",
        "    lower_case = t.lower()\n",
        "    del_pic = re.sub(linkimage, '', lower_case)\n",
        "    del_linkH = re.sub(linkpatternH, '', del_pic)\n",
        "    del_linkW = re.sub(linkpatternW, '', del_linkH)\n",
        "    del_amp = BeautifulSoup(del_linkW, 'lxml')\n",
        "    del_amp_text = del_amp.get_text()\n",
        "    del_link_mentions = re.sub(mention, '', del_amp_text)\n",
        "    del_hashtags = re.sub(mention, '', del_link_mentions)\n",
        "    del_punctuation = re.sub(punctuation, '', del_hashtags)\n",
        "    del_numbers = re.sub(numbers, '', del_punctuation)\n",
        "    del_emoticons = re.sub(regex_pattern, '', del_numbers)\n",
        "    del_emoji = re.sub(emojipattern, '', del_emoticons)\n",
        "\n",
        "    words = token.tokenize(del_emoji)\n",
        "    result_words = [x for x in words if len(x) > 2]\n",
        "\n",
        "    return (\" \".join(result_words)).strip()\n",
        "  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "OJAmtO-rLjda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gdxvi0KrC-Ch"
      },
      "source": [
        "# Reader function to read our stopwords list in an appropriate way\n",
        "def readFile(fileName):\n",
        "        fileObj = open(fileName, \"r\") #opens the file in read mode\n",
        "        words = fileObj.read().splitlines() #puts the file into an array\n",
        "        fileObj.close()\n",
        "        return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1p_95nDIwM9"
      },
      "source": [
        "# Assignment of the stopwords list with our additional list. \n",
        "stopwordslist = set(stopwords.words(\"english\"))\n",
        "ad_words = readFile('drive/MyDrive/WSU/Twitter/stopwords.txt')\n",
        "stopwordslist.update(ad_words)\n",
        "stopwordslist.update(stopwords.words(\"russian\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lYWoVcZ_Qzn"
      },
      "source": [
        "# Function to write the outputs for the top 50 words in a correct and readable manner.\n",
        "def writeToCSV(dict, j):\n",
        "  a_file = open(\"drive/MyDrive/WSU/Twitter/Andrei/Top50/April{}.txt\".format(j+1), \"w\")\n",
        "\n",
        "  writer = csv.writer(a_file)\n",
        "  for key, value in dict.items():\n",
        "    writer.writerow([key, value])\n",
        "\n",
        "  a_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dYfopi5kY8X"
      },
      "source": [
        "# This is the main body. Here we combine every previous function.\n",
        "# We also add an implementation of the sentiment analysis by using \n",
        "# vds.polarity_scores() function which returns 4 parameters:\n",
        "# negative, positive and neutral evaluation and a compose function value.\n",
        "# After every tweet is processed we make wordclouds for each day and get top50\n",
        "# words used in each day. And, of course, we always write the results into a file.\n",
        "\n",
        "for j in range(16):\n",
        "  sentiments = []\n",
        "  twitsDays[j] = separate(twits, j)\n",
        "  print(\"Cleaning the tweets for day {}...\\n\".format(j+1))\n",
        "  twitsCleaned[j] = []\n",
        "  for i in range(twitsDays[j].shape[0]):\n",
        "    if( (i+1)%100000 == 0 ):\n",
        "        print(\"Tweets {} of {} have ben processed\".format(i+1,twitsDays[j].shape[0]))  \n",
        "\n",
        "    x = clean((twitsDays[j].Text[i]))                                                               \n",
        "    twitsCleaned[j].append(x)\n",
        "    score_f = list(vds.polarity_scores(x).values())\n",
        "    if (score_f[3] > 0):\n",
        "      sent = 'pos'\n",
        "    elif (score_f[3] == 0):\n",
        "      sent = 'neu'\n",
        "    else:\n",
        "      sent = 'neg'\n",
        "    sentiments.append(sent)\n",
        "  \n",
        "  twitsDays[j]['Sentiments'] = sentiments\n",
        "  twitsDays[j].to_csv(\"drive/MyDrive/WSU/Twitter/Andrei/DaysByTweets/April{}.csv\".format(j+1))\n",
        "  string[j] = pd.Series(twitsCleaned[j]).str.cat(sep=' ')\n",
        "  wordcloud = []\n",
        "  wcpt = []\n",
        "  wcpt = WordCloud(width=1600, stopwords=stopwordslist,height=800,max_font_size=200,max_words=100,collocations=False, background_color='black').process_text(string[j])\n",
        "  top50[j] = dict(sorted(wcpt.items(), key=lambda item: item[1], reverse = True)[:50])\n",
        "  writeToCSV(top50[j], j)\n",
        "  wordcloud = WordCloud(width=1600, stopwords=stopwordslist,height=800,max_font_size=200,max_words=100,collocations=False, background_color='black').generate(string[j])\n",
        "  wordcloud.to_file(\"drive/MyDrive/WSU/Twitter/Andrei/WordClouds/April{}.png\".format(j+1))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}